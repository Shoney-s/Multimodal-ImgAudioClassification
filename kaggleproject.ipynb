{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Context**\n",
    "\n",
    "Blablabla kaggle dataset lien https://www.kaggle.com/birdy654/scene-classification-images-and-audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10351, 104)\n",
      "(3450, 104)\n",
      "(3451, 104)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IMAGE</th>\n",
       "      <th>mfcc_1</th>\n",
       "      <th>mfcc_2</th>\n",
       "      <th>mfcc_3</th>\n",
       "      <th>mfcc_4</th>\n",
       "      <th>mfcc_5</th>\n",
       "      <th>mfcc_6</th>\n",
       "      <th>mfcc_7</th>\n",
       "      <th>mfcc_8</th>\n",
       "      <th>mfcc_9</th>\n",
       "      <th>...</th>\n",
       "      <th>mfcc_97</th>\n",
       "      <th>mfcc_98</th>\n",
       "      <th>mfcc_99</th>\n",
       "      <th>mfcc_100</th>\n",
       "      <th>mfcc_101</th>\n",
       "      <th>mfcc_102</th>\n",
       "      <th>mfcc_103</th>\n",
       "      <th>mfcc_104</th>\n",
       "      <th>CLASS1</th>\n",
       "      <th>CLASS2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>images/forest/forest0.png</td>\n",
       "      <td>15.795384</td>\n",
       "      <td>-3.442518</td>\n",
       "      <td>-25.316836</td>\n",
       "      <td>-33.412104</td>\n",
       "      <td>2.447290</td>\n",
       "      <td>-46.981182</td>\n",
       "      <td>12.889984</td>\n",
       "      <td>-23.588534</td>\n",
       "      <td>-22.625879</td>\n",
       "      <td>...</td>\n",
       "      <td>-43.876462</td>\n",
       "      <td>20.697491</td>\n",
       "      <td>-22.793173</td>\n",
       "      <td>-9.417196</td>\n",
       "      <td>13.762870</td>\n",
       "      <td>-31.976786</td>\n",
       "      <td>18.461561</td>\n",
       "      <td>-13.140673</td>\n",
       "      <td>OUTDOORS</td>\n",
       "      <td>FOREST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>images/forest/forest1.png</td>\n",
       "      <td>15.883880</td>\n",
       "      <td>-3.494075</td>\n",
       "      <td>-21.189490</td>\n",
       "      <td>-18.077115</td>\n",
       "      <td>4.284962</td>\n",
       "      <td>-27.014271</td>\n",
       "      <td>3.666955</td>\n",
       "      <td>-9.091312</td>\n",
       "      <td>-3.746509</td>\n",
       "      <td>...</td>\n",
       "      <td>-33.883092</td>\n",
       "      <td>17.223236</td>\n",
       "      <td>-24.985005</td>\n",
       "      <td>12.035913</td>\n",
       "      <td>8.321000</td>\n",
       "      <td>-16.249293</td>\n",
       "      <td>8.717523</td>\n",
       "      <td>0.743640</td>\n",
       "      <td>OUTDOORS</td>\n",
       "      <td>FOREST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>images/forest/forest2.png</td>\n",
       "      <td>17.872629</td>\n",
       "      <td>-18.877467</td>\n",
       "      <td>-31.665319</td>\n",
       "      <td>-47.045579</td>\n",
       "      <td>1.813430</td>\n",
       "      <td>-45.899877</td>\n",
       "      <td>14.975982</td>\n",
       "      <td>-24.462396</td>\n",
       "      <td>-1.812962</td>\n",
       "      <td>...</td>\n",
       "      <td>-34.456028</td>\n",
       "      <td>21.433239</td>\n",
       "      <td>-14.190274</td>\n",
       "      <td>-8.629235</td>\n",
       "      <td>1.035640</td>\n",
       "      <td>-20.703358</td>\n",
       "      <td>5.986662</td>\n",
       "      <td>-14.644013</td>\n",
       "      <td>OUTDOORS</td>\n",
       "      <td>FOREST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>images/forest/forest3.png</td>\n",
       "      <td>16.843997</td>\n",
       "      <td>-3.527753</td>\n",
       "      <td>-21.282970</td>\n",
       "      <td>-24.248141</td>\n",
       "      <td>27.201589</td>\n",
       "      <td>-18.787674</td>\n",
       "      <td>30.093938</td>\n",
       "      <td>-1.922008</td>\n",
       "      <td>10.156418</td>\n",
       "      <td>...</td>\n",
       "      <td>-36.410615</td>\n",
       "      <td>19.949251</td>\n",
       "      <td>-5.466172</td>\n",
       "      <td>6.480569</td>\n",
       "      <td>13.070739</td>\n",
       "      <td>-14.853299</td>\n",
       "      <td>10.243606</td>\n",
       "      <td>-17.983957</td>\n",
       "      <td>OUTDOORS</td>\n",
       "      <td>FOREST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>images/forest/forest4.png</td>\n",
       "      <td>16.128583</td>\n",
       "      <td>-4.267328</td>\n",
       "      <td>-25.608325</td>\n",
       "      <td>-20.231084</td>\n",
       "      <td>15.922823</td>\n",
       "      <td>-35.703313</td>\n",
       "      <td>16.307644</td>\n",
       "      <td>-3.547505</td>\n",
       "      <td>4.804142</td>\n",
       "      <td>...</td>\n",
       "      <td>-41.548915</td>\n",
       "      <td>15.697646</td>\n",
       "      <td>-20.615005</td>\n",
       "      <td>-11.942869</td>\n",
       "      <td>5.421639</td>\n",
       "      <td>-27.445147</td>\n",
       "      <td>9.060233</td>\n",
       "      <td>-15.077528</td>\n",
       "      <td>OUTDOORS</td>\n",
       "      <td>FOREST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>images/forest/forest5.png</td>\n",
       "      <td>21.689841</td>\n",
       "      <td>-15.971450</td>\n",
       "      <td>-26.837817</td>\n",
       "      <td>-46.561006</td>\n",
       "      <td>20.770073</td>\n",
       "      <td>-8.153000</td>\n",
       "      <td>16.801556</td>\n",
       "      <td>-4.589764</td>\n",
       "      <td>7.219863</td>\n",
       "      <td>...</td>\n",
       "      <td>-21.613514</td>\n",
       "      <td>42.223289</td>\n",
       "      <td>-11.695203</td>\n",
       "      <td>2.910106</td>\n",
       "      <td>-35.891702</td>\n",
       "      <td>-2.755247</td>\n",
       "      <td>-2.448610</td>\n",
       "      <td>13.279929</td>\n",
       "      <td>OUTDOORS</td>\n",
       "      <td>FOREST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>images/forest/forest6.png</td>\n",
       "      <td>16.123850</td>\n",
       "      <td>-0.502807</td>\n",
       "      <td>-31.532553</td>\n",
       "      <td>-26.753161</td>\n",
       "      <td>22.110188</td>\n",
       "      <td>-44.973076</td>\n",
       "      <td>35.949924</td>\n",
       "      <td>-20.104215</td>\n",
       "      <td>-12.638088</td>\n",
       "      <td>...</td>\n",
       "      <td>-53.407842</td>\n",
       "      <td>32.373979</td>\n",
       "      <td>-28.167140</td>\n",
       "      <td>-7.078691</td>\n",
       "      <td>5.199906</td>\n",
       "      <td>-50.789380</td>\n",
       "      <td>5.619568</td>\n",
       "      <td>-30.189872</td>\n",
       "      <td>OUTDOORS</td>\n",
       "      <td>FOREST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>images/forest/forest7.png</td>\n",
       "      <td>15.871565</td>\n",
       "      <td>-0.763786</td>\n",
       "      <td>-24.201742</td>\n",
       "      <td>-24.479100</td>\n",
       "      <td>12.975591</td>\n",
       "      <td>-41.588451</td>\n",
       "      <td>26.818111</td>\n",
       "      <td>-6.913483</td>\n",
       "      <td>-4.095325</td>\n",
       "      <td>...</td>\n",
       "      <td>-37.867910</td>\n",
       "      <td>33.380044</td>\n",
       "      <td>-22.327289</td>\n",
       "      <td>-11.202320</td>\n",
       "      <td>-10.254014</td>\n",
       "      <td>-26.236774</td>\n",
       "      <td>4.476926</td>\n",
       "      <td>-4.142449</td>\n",
       "      <td>OUTDOORS</td>\n",
       "      <td>FOREST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>images/forest/forest8.png</td>\n",
       "      <td>17.764418</td>\n",
       "      <td>-4.284194</td>\n",
       "      <td>-27.631842</td>\n",
       "      <td>-49.291224</td>\n",
       "      <td>12.780080</td>\n",
       "      <td>-43.668120</td>\n",
       "      <td>34.199202</td>\n",
       "      <td>-25.522677</td>\n",
       "      <td>10.561731</td>\n",
       "      <td>...</td>\n",
       "      <td>-39.800392</td>\n",
       "      <td>7.948283</td>\n",
       "      <td>-31.925772</td>\n",
       "      <td>8.836070</td>\n",
       "      <td>-18.177841</td>\n",
       "      <td>-10.745605</td>\n",
       "      <td>-16.128549</td>\n",
       "      <td>5.578362</td>\n",
       "      <td>OUTDOORS</td>\n",
       "      <td>FOREST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>images/forest/forest9.png</td>\n",
       "      <td>17.008391</td>\n",
       "      <td>-7.689599</td>\n",
       "      <td>-23.670928</td>\n",
       "      <td>-35.503255</td>\n",
       "      <td>16.394963</td>\n",
       "      <td>-54.073955</td>\n",
       "      <td>22.142703</td>\n",
       "      <td>-24.226244</td>\n",
       "      <td>0.829439</td>\n",
       "      <td>...</td>\n",
       "      <td>-40.544998</td>\n",
       "      <td>13.595717</td>\n",
       "      <td>-19.715312</td>\n",
       "      <td>-8.342403</td>\n",
       "      <td>-14.912854</td>\n",
       "      <td>-24.710224</td>\n",
       "      <td>9.749967</td>\n",
       "      <td>-10.477606</td>\n",
       "      <td>OUTDOORS</td>\n",
       "      <td>FOREST</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 107 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       IMAGE     mfcc_1     mfcc_2     mfcc_3     mfcc_4  \\\n",
       "0  images/forest/forest0.png  15.795384  -3.442518 -25.316836 -33.412104   \n",
       "1  images/forest/forest1.png  15.883880  -3.494075 -21.189490 -18.077115   \n",
       "2  images/forest/forest2.png  17.872629 -18.877467 -31.665319 -47.045579   \n",
       "3  images/forest/forest3.png  16.843997  -3.527753 -21.282970 -24.248141   \n",
       "4  images/forest/forest4.png  16.128583  -4.267328 -25.608325 -20.231084   \n",
       "5  images/forest/forest5.png  21.689841 -15.971450 -26.837817 -46.561006   \n",
       "6  images/forest/forest6.png  16.123850  -0.502807 -31.532553 -26.753161   \n",
       "7  images/forest/forest7.png  15.871565  -0.763786 -24.201742 -24.479100   \n",
       "8  images/forest/forest8.png  17.764418  -4.284194 -27.631842 -49.291224   \n",
       "9  images/forest/forest9.png  17.008391  -7.689599 -23.670928 -35.503255   \n",
       "\n",
       "      mfcc_5     mfcc_6     mfcc_7     mfcc_8     mfcc_9   ...      mfcc_97  \\\n",
       "0   2.447290 -46.981182  12.889984 -23.588534 -22.625879   ...   -43.876462   \n",
       "1   4.284962 -27.014271   3.666955  -9.091312  -3.746509   ...   -33.883092   \n",
       "2   1.813430 -45.899877  14.975982 -24.462396  -1.812962   ...   -34.456028   \n",
       "3  27.201589 -18.787674  30.093938  -1.922008  10.156418   ...   -36.410615   \n",
       "4  15.922823 -35.703313  16.307644  -3.547505   4.804142   ...   -41.548915   \n",
       "5  20.770073  -8.153000  16.801556  -4.589764   7.219863   ...   -21.613514   \n",
       "6  22.110188 -44.973076  35.949924 -20.104215 -12.638088   ...   -53.407842   \n",
       "7  12.975591 -41.588451  26.818111  -6.913483  -4.095325   ...   -37.867910   \n",
       "8  12.780080 -43.668120  34.199202 -25.522677  10.561731   ...   -39.800392   \n",
       "9  16.394963 -54.073955  22.142703 -24.226244   0.829439   ...   -40.544998   \n",
       "\n",
       "     mfcc_98    mfcc_99   mfcc_100   mfcc_101   mfcc_102   mfcc_103  \\\n",
       "0  20.697491 -22.793173  -9.417196  13.762870 -31.976786  18.461561   \n",
       "1  17.223236 -24.985005  12.035913   8.321000 -16.249293   8.717523   \n",
       "2  21.433239 -14.190274  -8.629235   1.035640 -20.703358   5.986662   \n",
       "3  19.949251  -5.466172   6.480569  13.070739 -14.853299  10.243606   \n",
       "4  15.697646 -20.615005 -11.942869   5.421639 -27.445147   9.060233   \n",
       "5  42.223289 -11.695203   2.910106 -35.891702  -2.755247  -2.448610   \n",
       "6  32.373979 -28.167140  -7.078691   5.199906 -50.789380   5.619568   \n",
       "7  33.380044 -22.327289 -11.202320 -10.254014 -26.236774   4.476926   \n",
       "8   7.948283 -31.925772   8.836070 -18.177841 -10.745605 -16.128549   \n",
       "9  13.595717 -19.715312  -8.342403 -14.912854 -24.710224   9.749967   \n",
       "\n",
       "    mfcc_104    CLASS1  CLASS2  \n",
       "0 -13.140673  OUTDOORS  FOREST  \n",
       "1   0.743640  OUTDOORS  FOREST  \n",
       "2 -14.644013  OUTDOORS  FOREST  \n",
       "3 -17.983957  OUTDOORS  FOREST  \n",
       "4 -15.077528  OUTDOORS  FOREST  \n",
       "5  13.279929  OUTDOORS  FOREST  \n",
       "6 -30.189872  OUTDOORS  FOREST  \n",
       "7  -4.142449  OUTDOORS  FOREST  \n",
       "8   5.578362  OUTDOORS  FOREST  \n",
       "9 -10.477606  OUTDOORS  FOREST  \n",
       "\n",
       "[10 rows x 107 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/dataset.csv', delimiter=',', nrows=None)\n",
    "data_train = np.array(data)\n",
    "\n",
    "audio = data_train[:,1:-2] #last index of the interval isn't included in the range : CLASS1\n",
    "labels = data_train[:,-1]\n",
    "img_paths = data['IMAGE']\n",
    "\n",
    "img_train, img_temp, audio_train, audio_temp, labels_train, labels_temp = train_test_split(img_paths, audio, labels, train_size=0.6)\n",
    "img_val, img_test, audio_val, audio_test, labels_val, labels_test = train_test_split(img_temp, audio_temp, labels_temp, train_size=0.5)\n",
    "\n",
    "print(np.shape(audio_train))\n",
    "print(np.shape(audio_val))\n",
    "print(np.shape(audio_test))\n",
    "data.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On définit les seeds qui permettent de générer aléatoirement les mêmes nombres, et donc rendre les résultats reproductibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pytorch Dataset**\n",
    "\n",
    "On créé une classe Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_type, root_dir, img_data, audio_data, labels=None, img_transform=None, audio_transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.img_data = img_data\n",
    "        self.audio_data = audio_data\n",
    "        self.labels = labels\n",
    "        self.img_transform = img_transform\n",
    "        self.audio_transform = audio_transform\n",
    "        self.data_type = data_type\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_data)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        if self.data_type == \"img\":\n",
    "            img = os.path.join(self.root_dir, self.img_data.iloc[idx])\n",
    "            if self.img_transform:\n",
    "                img = self.img_transform(img)\n",
    "            audio = None\n",
    "            \n",
    "        elif self.data_type == \"audio\":\n",
    "            audio = self.audio_data[idx,:]\n",
    "            if self.audio_transform:\n",
    "                audio = self.audio_transform(audio)\n",
    "            img = None\n",
    "        \n",
    "        elif self.data_type == \"imgaudio\":\n",
    "            img = os.path.join(self.root_dir, self.img_paths.iloc[idx])\n",
    "            audio = self.audio_data[idx,:]\n",
    "            if self.img_transform:\n",
    "                img = self.img_transform(img)\n",
    "                audio = self.audio_transform(audio)\n",
    "\n",
    "        else:\n",
    "            raise ValueError('Data must be img, audio or imgaudio')\n",
    "                               \n",
    "        return ( data_type, img, audio if labels is None else  data_type, img, audio, self.labels[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BASELINE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain\\npredict\\n\\ndata augmentation\\nload data imag\\nget imagenet pretrained model --> compare feature extraction and transfer learning 2 last layers\\nuse train\\nuse predict\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from random import seed\n",
    "from random import randrange\n",
    "\n",
    "def zero_rule_algorithm_classification(train, test):\n",
    "    output_values = [row[-1] for row in train]\n",
    "    prediction = max(set(output_values), key=output_values.count)\n",
    "    predicted = [prediction for i in range(len(test))]\n",
    "    return predicted\n",
    "\n",
    "seed(1)\n",
    "\n",
    "predictions = zero_rule_algorithm_classification(data_train, data_train)\n",
    "#print(predictions)\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "train\n",
    "predict\n",
    "\n",
    "data augmentation\n",
    "load data imag\n",
    "get imagenet pretrained model --> compare feature extraction and transfer learning 2 last layers\n",
    "use train\n",
    "use predict\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRAIN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, val_loader, model, criterion, optimizer, scheduler, epochs=20):\n",
    "    \n",
    "    print(\"trainloader :\", train_loader[0], \"\\nval_loader :\",  val_loader[0])\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for index, data in enumerate(train_loader):\n",
    "            data_type, img, audio, labels = data\n",
    "            img, audio, labels = img.cuda(), audio.cuda(), labels.cuda()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "          \n",
    "        #We compute validation data accuracy on each epoch to prevent overfitting \n",
    "        #if val_accuracy isn't improved by current training epoch\n",
    "        val_acc = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad: #Validation data aim to test, not to train NN --> grad isn't needed\n",
    "            for dat_type, img, audio, imgaudio, labl in val_loader:\n",
    "                img, audio, labels = img.cuda(), audio.cuda(), labels.cuda()\n",
    "                if data_type == \"img\":\n",
    "                    outputs = model(img)\n",
    "                elif data_type == \"audio\":\n",
    "                    outputs = model(audio)\n",
    "                else:\n",
    "                    outputs = model(img, audio)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PREDICT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict():\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font size=\"6\" color=\"darkblue\">IMAGES</font>**\n",
    "\n",
    "Explication données image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMAGE VISUALIZATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DATA TRANSFORMATION** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# DA : Data Augmentation\n",
    "# DP : Data Preparation --> transform data to a more ergonomic data format\n",
    "\n",
    "img_train_transform = transforms.Compose([ #Compose is used to chain multiple transforms to create a transformation pipeline\n",
    "    transforms.RandomResizedCrop(224), #DA\n",
    "    transforms.RandomHorizontalFlip(), #DA\n",
    "    transforms.ToTensor(), #DP to compute on GPU\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225]) #DP\n",
    "])\n",
    "img_val_transform = transforms.Compose([\n",
    "    #transforms.RandomResizedCrop(224),\n",
    "    transforms.Resize(256), #DA fixed resize and crop for reliability\n",
    "    transforms.CenterCrop(224),# DA\n",
    "    transforms.ToTensor(), #DP\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225]) #DP\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMAGE LOADER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "data_type = 'img'\n",
    "train_data = CustomDataset(data_type=data_type, root_dir='/data', img_data=img_train, audio_data=audio_train, labels=labels_train, img_transform=img_train_transform)\n",
    "val_data = CustomDataset(data_type=data_type, root_dir='/data', img_data=img_val,  audio_data=audio_val,labels=labels_val, img_transform=img_val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRANSFER LEARNING** \n",
    "\n",
    "on charge le réseau préentrainé\n",
    "On utilise Imagenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-342c2532f8a4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mresnext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_ftrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mresnext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresnext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\université paris sud\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mcuda\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mModule\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \"\"\"\n\u001b[1;32m--> 491\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mxpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\université paris sud\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    385\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 387\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    388\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\université paris sud\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    407\u001b[0m                 \u001b[1;31m# `with torch.no_grad():`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 409\u001b[1;33m                     \u001b[0mparam_applied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    410\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\université paris sud\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mModule\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \"\"\"\n\u001b[1;32m--> 491\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mxpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\université paris sud\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[1;31m# This function throws if there's a driver initialization error, no GPUs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m         \u001b[1;31m# are found or any other error occurs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m         \u001b[1;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m         \u001b[1;31m# we need to just return without initializing in that case.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "import torchvision.models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "resnext = torchvision.models.resnext50_32x4d(pretrained=True, progress=True)\n",
    "\n",
    "for param in resnext.parameters(): # on ne calcule les gradients que pour la dernière couche\n",
    "    param.requires_grad = False\n",
    "    \n",
    "num_ftrs = resnext.fc.in_features\n",
    "resnext.fc = nn.Linear(num_ftrs, 9)\n",
    "\n",
    "resnext = resnext.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(resnext.fc.parameters(), lr=0.001) # on optimise seulement les coeffs de la dernière couche\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, patience=0,factor=0.5) # permet de réduire le learning rate quand la loss ne diminue pas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train() got an unexpected keyword argument 'num_epochs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-dfb8f33a0bd3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mresnext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_acc_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_acc_vals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresnext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'image'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: train() got an unexpected keyword argument 'num_epochs'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_epochs = 30\n",
    "resnext, loss_vals, train_acc_vals, val_acc_vals = train(resnext, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=num_epochs, input_type='image')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font size=\"6\" color=\"darkblue\">AUDIO</font>**\n",
    "\n",
    "Type de données MCCF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AUDIO DATA VISUALIZATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AUDIO DATA TRANSFORMATION** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "audio_train_transform = transforms.Compose([\n",
    "    \n",
    "])\n",
    "audio_val_transform = transforms.Compose([\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AUDIO DATA LOADER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = 'audio'\n",
    "train_data = CustomDataset(data_type=data_type, root_dir='/data', img_data=img_train, audio_data=audio_train, labels=labels_train, audio_transform=audio_train_transform)\n",
    "val_data = CustomDataset(data_type=data_type, root_dir='/data', img_data=img_val, audio_data=audio_val, labels=labels_val, audio_transform=audio_val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
