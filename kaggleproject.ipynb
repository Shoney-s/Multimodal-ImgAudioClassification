{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-29T14:03:22.174370Z",
     "iopub.status.busy": "2021-05-29T14:03:22.173987Z",
     "iopub.status.idle": "2021-05-29T14:03:22.180934Z",
     "shell.execute_reply": "2021-05-29T14:03:22.179326Z",
     "shell.execute_reply.started": "2021-05-29T14:03:22.174336Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import random\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Context**\n",
    "\n",
    "Kaggle dataset link https://www.kaggle.com/birdy654/scene-classification-images-and-audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-29T14:26:51.892132Z",
     "iopub.status.busy": "2021-05-29T14:26:51.891736Z",
     "iopub.status.idle": "2021-05-29T14:26:52.609416Z",
     "shell.execute_reply": "2021-05-29T14:26:52.608106Z",
     "shell.execute_reply.started": "2021-05-29T14:26:51.892085Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IMAGE</th>\n",
       "      <th>mfcc_1</th>\n",
       "      <th>mfcc_2</th>\n",
       "      <th>mfcc_3</th>\n",
       "      <th>mfcc_4</th>\n",
       "      <th>mfcc_5</th>\n",
       "      <th>mfcc_6</th>\n",
       "      <th>mfcc_7</th>\n",
       "      <th>mfcc_8</th>\n",
       "      <th>mfcc_9</th>\n",
       "      <th>...</th>\n",
       "      <th>mfcc_97</th>\n",
       "      <th>mfcc_98</th>\n",
       "      <th>mfcc_99</th>\n",
       "      <th>mfcc_100</th>\n",
       "      <th>mfcc_101</th>\n",
       "      <th>mfcc_102</th>\n",
       "      <th>mfcc_103</th>\n",
       "      <th>mfcc_104</th>\n",
       "      <th>CLASS1</th>\n",
       "      <th>CLASS2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>images/forest/forest0.png</td>\n",
       "      <td>15.795384</td>\n",
       "      <td>-3.442518</td>\n",
       "      <td>-25.316836</td>\n",
       "      <td>-33.412104</td>\n",
       "      <td>2.447290</td>\n",
       "      <td>-46.981182</td>\n",
       "      <td>12.889984</td>\n",
       "      <td>-23.588534</td>\n",
       "      <td>-22.625879</td>\n",
       "      <td>...</td>\n",
       "      <td>-43.876462</td>\n",
       "      <td>20.697491</td>\n",
       "      <td>-22.793173</td>\n",
       "      <td>-9.417196</td>\n",
       "      <td>13.762870</td>\n",
       "      <td>-31.976786</td>\n",
       "      <td>18.461561</td>\n",
       "      <td>-13.140673</td>\n",
       "      <td>OUTDOORS</td>\n",
       "      <td>FOREST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>images/forest/forest1.png</td>\n",
       "      <td>15.883880</td>\n",
       "      <td>-3.494075</td>\n",
       "      <td>-21.189490</td>\n",
       "      <td>-18.077115</td>\n",
       "      <td>4.284962</td>\n",
       "      <td>-27.014271</td>\n",
       "      <td>3.666955</td>\n",
       "      <td>-9.091312</td>\n",
       "      <td>-3.746509</td>\n",
       "      <td>...</td>\n",
       "      <td>-33.883092</td>\n",
       "      <td>17.223236</td>\n",
       "      <td>-24.985005</td>\n",
       "      <td>12.035913</td>\n",
       "      <td>8.321000</td>\n",
       "      <td>-16.249293</td>\n",
       "      <td>8.717523</td>\n",
       "      <td>0.743640</td>\n",
       "      <td>OUTDOORS</td>\n",
       "      <td>FOREST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>images/forest/forest2.png</td>\n",
       "      <td>17.872629</td>\n",
       "      <td>-18.877467</td>\n",
       "      <td>-31.665319</td>\n",
       "      <td>-47.045579</td>\n",
       "      <td>1.813430</td>\n",
       "      <td>-45.899877</td>\n",
       "      <td>14.975982</td>\n",
       "      <td>-24.462396</td>\n",
       "      <td>-1.812962</td>\n",
       "      <td>...</td>\n",
       "      <td>-34.456028</td>\n",
       "      <td>21.433239</td>\n",
       "      <td>-14.190274</td>\n",
       "      <td>-8.629235</td>\n",
       "      <td>1.035640</td>\n",
       "      <td>-20.703358</td>\n",
       "      <td>5.986662</td>\n",
       "      <td>-14.644013</td>\n",
       "      <td>OUTDOORS</td>\n",
       "      <td>FOREST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>images/forest/forest3.png</td>\n",
       "      <td>16.843997</td>\n",
       "      <td>-3.527753</td>\n",
       "      <td>-21.282970</td>\n",
       "      <td>-24.248141</td>\n",
       "      <td>27.201589</td>\n",
       "      <td>-18.787674</td>\n",
       "      <td>30.093938</td>\n",
       "      <td>-1.922008</td>\n",
       "      <td>10.156418</td>\n",
       "      <td>...</td>\n",
       "      <td>-36.410615</td>\n",
       "      <td>19.949251</td>\n",
       "      <td>-5.466172</td>\n",
       "      <td>6.480569</td>\n",
       "      <td>13.070739</td>\n",
       "      <td>-14.853299</td>\n",
       "      <td>10.243606</td>\n",
       "      <td>-17.983957</td>\n",
       "      <td>OUTDOORS</td>\n",
       "      <td>FOREST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>images/forest/forest4.png</td>\n",
       "      <td>16.128583</td>\n",
       "      <td>-4.267328</td>\n",
       "      <td>-25.608325</td>\n",
       "      <td>-20.231084</td>\n",
       "      <td>15.922823</td>\n",
       "      <td>-35.703313</td>\n",
       "      <td>16.307644</td>\n",
       "      <td>-3.547505</td>\n",
       "      <td>4.804142</td>\n",
       "      <td>...</td>\n",
       "      <td>-41.548915</td>\n",
       "      <td>15.697646</td>\n",
       "      <td>-20.615005</td>\n",
       "      <td>-11.942869</td>\n",
       "      <td>5.421639</td>\n",
       "      <td>-27.445147</td>\n",
       "      <td>9.060233</td>\n",
       "      <td>-15.077528</td>\n",
       "      <td>OUTDOORS</td>\n",
       "      <td>FOREST</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 107 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       IMAGE     mfcc_1     mfcc_2     mfcc_3     mfcc_4  \\\n",
       "0  images/forest/forest0.png  15.795384  -3.442518 -25.316836 -33.412104   \n",
       "1  images/forest/forest1.png  15.883880  -3.494075 -21.189490 -18.077115   \n",
       "2  images/forest/forest2.png  17.872629 -18.877467 -31.665319 -47.045579   \n",
       "3  images/forest/forest3.png  16.843997  -3.527753 -21.282970 -24.248141   \n",
       "4  images/forest/forest4.png  16.128583  -4.267328 -25.608325 -20.231084   \n",
       "\n",
       "      mfcc_5     mfcc_6     mfcc_7     mfcc_8     mfcc_9  ...    mfcc_97  \\\n",
       "0   2.447290 -46.981182  12.889984 -23.588534 -22.625879  ... -43.876462   \n",
       "1   4.284962 -27.014271   3.666955  -9.091312  -3.746509  ... -33.883092   \n",
       "2   1.813430 -45.899877  14.975982 -24.462396  -1.812962  ... -34.456028   \n",
       "3  27.201589 -18.787674  30.093938  -1.922008  10.156418  ... -36.410615   \n",
       "4  15.922823 -35.703313  16.307644  -3.547505   4.804142  ... -41.548915   \n",
       "\n",
       "     mfcc_98    mfcc_99   mfcc_100   mfcc_101   mfcc_102   mfcc_103  \\\n",
       "0  20.697491 -22.793173  -9.417196  13.762870 -31.976786  18.461561   \n",
       "1  17.223236 -24.985005  12.035913   8.321000 -16.249293   8.717523   \n",
       "2  21.433239 -14.190274  -8.629235   1.035640 -20.703358   5.986662   \n",
       "3  19.949251  -5.466172   6.480569  13.070739 -14.853299  10.243606   \n",
       "4  15.697646 -20.615005 -11.942869   5.421639 -27.445147   9.060233   \n",
       "\n",
       "    mfcc_104    CLASS1  CLASS2  \n",
       "0 -13.140673  OUTDOORS  FOREST  \n",
       "1   0.743640  OUTDOORS  FOREST  \n",
       "2 -14.644013  OUTDOORS  FOREST  \n",
       "3 -17.983957  OUTDOORS  FOREST  \n",
       "4 -15.077528  OUTDOORS  FOREST  \n",
       "\n",
       "[5 rows x 107 columns]"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('/kaggle/input/scene-classification-images-and-audio/dataset.csv', delimiter=',', nrows=None)\n",
    "data_train = np.array(data)\n",
    "\n",
    "audio = data_train[:,1:-2].astype('float32') #last index of the interval isn't included in the range : CLASS1\n",
    "labels = data_train[:,-1]\n",
    "img_paths = data['IMAGE']\n",
    "\n",
    "classes = [\"FOREST\", \"CLASSROOM\", \"CITY\", \"RIVER\", \"GROCERY-STORE\",\"JUNGLE\",\"BEACH\",\"FOOTBALL-MATCH\",\"RESTAURANT\"]\n",
    "for index,class_name in enumerate(classes):\n",
    "    labels = np.where(labels == class_name, index, labels)\n",
    "\n",
    "labels.astype('int32')\n",
    "\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-29T14:03:22.951425Z",
     "iopub.status.busy": "2021-05-29T14:03:22.950908Z",
     "iopub.status.idle": "2021-05-29T14:03:22.974788Z",
     "shell.execute_reply": "2021-05-29T14:03:22.973829Z",
     "shell.execute_reply.started": "2021-05-29T14:03:22.951380Z"
    }
   },
   "outputs": [],
   "source": [
    "img_train, img_temp, audio_train, audio_temp, labels_train, labels_temp = train_test_split(img_paths, audio, labels, train_size=0.6)\n",
    "img_val, img_test, audio_val, audio_test, labels_val, labels_test = train_test_split(img_temp, audio_temp, labels_temp, train_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the seeds that allows to generate randomly the same numbers, thus making results reproducible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-29T14:03:22.978329Z",
     "iopub.status.busy": "2021-05-29T14:03:22.977889Z",
     "iopub.status.idle": "2021-05-29T14:03:22.983840Z",
     "shell.execute_reply": "2021-05-29T14:03:22.982440Z",
     "shell.execute_reply.started": "2021-05-29T14:03:22.978290Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pytorch Dataset**\n",
    "\n",
    "We create Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-29T14:03:22.987461Z",
     "iopub.status.busy": "2021-05-29T14:03:22.986879Z",
     "iopub.status.idle": "2021-05-29T14:03:23.000278Z",
     "shell.execute_reply": "2021-05-29T14:03:22.999102Z",
     "shell.execute_reply.started": "2021-05-29T14:03:22.987411Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "class ImgAudioDataset(Dataset):\n",
    "    def __init__(self, root_dir, img_data, audio_data, labels=None, img_transform=None, audio_transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.img_data = img_data\n",
    "        self.audio_data = audio_data\n",
    "        self.labels = labels\n",
    "        self.img_transform = img_transform\n",
    "        self.audio_transform = audio_transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_data)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(os.path.join(self.root_dir, self.img_data.iloc[idx]))\n",
    "        audio = self.audio_data[idx,:]\n",
    "        if self.img_transform:\n",
    "            img = self.img_transform(img)\n",
    "        if self.audio_transform:\n",
    "            audio = self.audio_transform(audio)        \n",
    "                               \n",
    "        return ((img, audio) if labels is None else (img, audio, int(self.labels[idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRAIN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-29T14:03:23.002637Z",
     "iopub.status.busy": "2021-05-29T14:03:23.002161Z",
     "iopub.status.idle": "2021-05-29T14:03:23.021388Z",
     "shell.execute_reply": "2021-05-29T14:03:23.019630Z",
     "shell.execute_reply.started": "2021-05-29T14:03:23.002595Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "def train(data_type, model, train_loader, val_loader, criterion, optimizer, scheduler, epochs):\n",
    "    i = 1\n",
    "    train_acc_vals = []\n",
    "    train_loss_vals = []\n",
    "    val_acc_vals = []\n",
    "    \n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        print(\"epoch : \" + str(i))\n",
    "        i+=1\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0.0\n",
    "        epoch_loss_sum = 0.0\n",
    "        epoch_loss = 0.0\n",
    "        epoch_acc = 0.0\n",
    "\n",
    "        model.train()\n",
    "        for index, data in enumerate(train_loader):\n",
    "            img, audio, labels = data\n",
    "            img, audio, labels = img.cuda(), audio.cuda(), labels.cuda()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with autocast():\n",
    "                # forward\n",
    "                if data_type == \"img\":\n",
    "                    outputs = model(img)\n",
    "                elif data_type == \"audio\":\n",
    "                    outputs = model(audio)\n",
    "                elif data_type == \"imgaudio\":\n",
    "                    outputs = model(img, audio)\n",
    "                else:\n",
    "                    raise ValueError('Data must be img, audio or imgaudio')\n",
    "                loss = criterion(outputs,labels) # compare obtained and expected output\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                running_corrects += torch.sum(preds == labels)\n",
    "                epoch_loss_sum += loss.item()\n",
    "                \n",
    "            # scale aim to prevent zero flush of low magnitude gradient values\n",
    "            scaler.scale(loss).backward() # backward\n",
    "            scaler.step(optimizer) # compute gradient for optimization\n",
    "            scaler.update()\n",
    "            \n",
    "            #print statistics\n",
    "            running_loss += loss.item()\n",
    "            if index % 200 == 199:    \n",
    "                print('[%d, %5d] loss: %.3f' % (epoch + 1, index + 1, running_loss / 200))\n",
    "                running_loss = 0.0\n",
    "                \n",
    "        best_model = copy.deepcopy(model.state_dict())\n",
    "                 \n",
    "        epoch_loss = epoch_loss_sum / len(train_loader.dataset)\n",
    "        epoch_acc = running_corrects / len(train_loader.dataset)\n",
    "        \n",
    "        print(f'Epoch accuracy: {epoch_acc}, Epoch loss: {epoch_loss}')\n",
    "        \n",
    "        train_loss_vals.append(epoch_acc)\n",
    "        train_acc_vals.append(epoch_loss)\n",
    "            \n",
    "    \n",
    "        \"\"\"\n",
    "            #We compute validation data accuracy on each epoch to prevent overfitting \n",
    "            #if val_accuracy isn't improved by current training epoch\n",
    "            val_acc = 0\n",
    "            model.eval()\n",
    "            with torch.no_grad: #Validation data aim to test, not to train NN --> grad isn't needed\n",
    "                for img, audio, labels in val_loader:\n",
    "                    img, audio, labels = img.cuda(), audio.cuda(), labels.cuda()\n",
    "                    if data_type == \"img\":\n",
    "                        outputs = model(img)\n",
    "                    elif data_type == \"audio\":\n",
    "                        outputs = model(audio)\n",
    "                    else:\n",
    "                        outputs = model(img, audio)\n",
    "                        \"\"\"\n",
    "        print(f'Epoch duration : {time.time() - epoch_start}')\n",
    "        model.load_state_dict(best_model)\n",
    "    return model, train_loss_vals, train_acc_vals, val_acc_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PREDICT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-29T14:03:23.024043Z",
     "iopub.status.busy": "2021-05-29T14:03:23.023426Z",
     "iopub.status.idle": "2021-05-29T14:03:23.036191Z",
     "shell.execute_reply": "2021-05-29T14:03:23.034855Z",
     "shell.execute_reply.started": "2021-05-29T14:03:23.023984Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(test_loader, model):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad: #Validation data aim to test, not to train NN --> grad isn't needed\n",
    "        for data_type, img, audio, labels in test_loader:\n",
    "            img, audio = img.cuda(), audio.cuda()\n",
    "        if data_type == \"img\":\n",
    "            outputs = model(img)\n",
    "        elif data_type == \"audio\":\n",
    "            outputs = model(audio)\n",
    "        else:\n",
    "             outputs = model(img, audio)\n",
    "                \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font size=\"6\" color=\"darkblue\">IMAGES</font>**\n",
    "\n",
    "Explication données image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMAGE VISUALIZATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DATA TRANSFORMATION** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-29T14:03:23.038519Z",
     "iopub.status.busy": "2021-05-29T14:03:23.037871Z",
     "iopub.status.idle": "2021-05-29T14:03:23.053151Z",
     "shell.execute_reply": "2021-05-29T14:03:23.051914Z",
     "shell.execute_reply.started": "2021-05-29T14:03:23.038325Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# DA : Data Augmentation\n",
    "# DP : Data Preparation --> transform data to a more ergonomic data format\n",
    "\n",
    "img_train_transform = transforms.Compose([ #Compose is used to chain multiple transforms to create a transformation pipeline\n",
    "    transforms.RandomResizedCrop(224), #DA\n",
    "    transforms.RandomHorizontalFlip(), #DA\n",
    "    transforms.ToTensor(), #DP to compute on GPU\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225]) #DP\n",
    "])\n",
    "img_val_transform = transforms.Compose([\n",
    "    transforms.Resize(256), #DA fixed resize and crop for reliability\n",
    "    transforms.CenterCrop(224),# DA\n",
    "    transforms.ToTensor(), #DP\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225]) #DP\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMAGE LOADER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-29T14:03:23.057001Z",
     "iopub.status.busy": "2021-05-29T14:03:23.055974Z",
     "iopub.status.idle": "2021-05-29T14:03:23.065507Z",
     "shell.execute_reply": "2021-05-29T14:03:23.064338Z",
     "shell.execute_reply.started": "2021-05-29T14:03:23.056955Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "train_data = ImgAudioDataset(root_dir='/kaggle/input/scene-classification-images-and-audio', img_data=img_train, audio_data=audio_train, labels=labels_train, img_transform=img_train_transform)\n",
    "val_data = ImgAudioDataset(root_dir='/kaggle/input/scene-classification-images-and-audio/', img_data=img_val,  audio_data=audio_val,labels=labels_val, img_transform=img_val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=16, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_data, batch_size=16, shuffle=True, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRANSFER LEARNING** \n",
    "\n",
    "We load a pretrained neural network on ImageNet dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-29T14:03:23.070143Z",
     "iopub.status.busy": "2021-05-29T14:03:23.069595Z",
     "iopub.status.idle": "2021-05-29T14:03:23.844170Z",
     "shell.execute_reply": "2021-05-29T14:03:23.843077Z",
     "shell.execute_reply.started": "2021-05-29T14:03:23.070110Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torchvision.models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "resnext = torchvision.models.resnext50_32x4d(pretrained=True, progress=True)\n",
    "\n",
    "for param in resnext.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "num_ftrs = resnext.fc.in_features\n",
    "resnext.fc = nn.Linear(num_ftrs, 9)\n",
    "\n",
    "resnext = resnext.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-29T14:03:23.848387Z",
     "iopub.status.busy": "2021-05-29T14:03:23.848096Z",
     "iopub.status.idle": "2021-05-29T14:03:23.857325Z",
     "shell.execute_reply": "2021-05-29T14:03:23.856106Z",
     "shell.execute_reply.started": "2021-05-29T14:03:23.848359Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(resnext.fc.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, patience=0,factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-29T14:03:23.860168Z",
     "iopub.status.busy": "2021-05-29T14:03:23.859586Z",
     "iopub.status.idle": "2021-05-29T14:15:05.260882Z",
     "shell.execute_reply": "2021-05-29T14:15:05.259687Z",
     "shell.execute_reply.started": "2021-05-29T14:03:23.860126Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1\n",
      "[1,   200] loss: 0.693\n",
      "[1,   400] loss: 0.325\n",
      "[1,   600] loss: 0.262\n",
      "Epoch accuracy: 0.8791421055793762, Epoch loss: 0.02611831506459958\n",
      "Epoch duration : 70.44312930107117\n",
      "epoch : 2\n",
      "[2,   200] loss: 0.233\n",
      "[2,   400] loss: 0.217\n",
      "[2,   600] loss: 0.217\n",
      "Epoch accuracy: 0.9320838451385498, Epoch loss: 0.013718721404280822\n",
      "Epoch duration : 69.65638756752014\n",
      "epoch : 3\n",
      "[3,   200] loss: 0.200\n",
      "[3,   400] loss: 0.190\n",
      "[3,   600] loss: 0.182\n",
      "Epoch accuracy: 0.9396193623542786, Epoch loss: 0.011769087775653124\n",
      "Epoch duration : 69.95826888084412\n",
      "epoch : 4\n",
      "[4,   200] loss: 0.201\n",
      "[4,   400] loss: 0.190\n",
      "[4,   600] loss: 0.179\n",
      "Epoch accuracy: 0.9363346695899963, Epoch loss: 0.011735761784362227\n",
      "Epoch duration : 70.77776789665222\n",
      "epoch : 5\n",
      "[5,   200] loss: 0.169\n",
      "[5,   400] loss: 0.179\n",
      "[5,   600] loss: 0.169\n",
      "Epoch accuracy: 0.9430972933769226, Epoch loss: 0.01087837010536909\n",
      "Epoch duration : 69.55069279670715\n",
      "epoch : 6\n",
      "[6,   200] loss: 0.165\n",
      "[6,   400] loss: 0.144\n",
      "[6,   600] loss: 0.162\n",
      "Epoch accuracy: 0.9475412964820862, Epoch loss: 0.009818606165303586\n",
      "Epoch duration : 70.4236650466919\n",
      "epoch : 7\n",
      "[7,   200] loss: 0.162\n",
      "[7,   400] loss: 0.148\n",
      "[7,   600] loss: 0.162\n",
      "Epoch accuracy: 0.9454159140586853, Epoch loss: 0.009942741829267797\n",
      "Epoch duration : 70.23795747756958\n",
      "epoch : 8\n",
      "[8,   200] loss: 0.157\n",
      "[8,   400] loss: 0.151\n",
      "[8,   600] loss: 0.157\n",
      "Epoch accuracy: 0.9463819861412048, Epoch loss: 0.00984918272790394\n",
      "Epoch duration : 70.25931525230408\n",
      "epoch : 9\n",
      "[9,   200] loss: 0.155\n",
      "[9,   400] loss: 0.161\n",
      "[9,   600] loss: 0.149\n",
      "Epoch accuracy: 0.9474446773529053, Epoch loss: 0.009842675273423545\n",
      "Epoch duration : 70.2033314704895\n",
      "epoch : 10\n",
      "[10,   200] loss: 0.132\n",
      "[10,   400] loss: 0.178\n",
      "[10,   600] loss: 0.150\n",
      "Epoch accuracy: 0.949763298034668, Epoch loss: 0.009627253809411509\n",
      "Epoch duration : 69.61997532844543\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = 10\n",
    "data_type = 'img'\n",
    "resnext, loss_vals, train_acc_vals, val_acc_vals = train(data_type, resnext, train_loader, val_loader, criterion, optimizer, scheduler, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font size=\"6\" color=\"darkblue\">AUDIO</font>**\n",
    "\n",
    "Type de données MCCF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AUDIO DATA VISUALIZATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-29T14:15:05.263454Z",
     "iopub.status.busy": "2021-05-29T14:15:05.262925Z",
     "iopub.status.idle": "2021-05-29T14:15:05.896733Z",
     "shell.execute_reply": "2021-05-29T14:15:05.895699Z",
     "shell.execute_reply.started": "2021-05-29T14:15:05.263406Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.hist(audio[::,0])\n",
    "plt.title('mfcc_1')\n",
    "plt.subplot(2,2,2)\n",
    "plt.hist(audio[::,1])\n",
    "plt.title('mfcc_2')\n",
    "plt.subplot(2,2,3)\n",
    "plt.hist(audio[::,2])\n",
    "plt.title('mfcc_3')\n",
    "plt.subplot(2,2,4)\n",
    "plt.hist(audio[::,3])\n",
    "plt.title('mfcc_4')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-29T14:15:05.898996Z",
     "iopub.status.busy": "2021-05-29T14:15:05.898728Z",
     "iopub.status.idle": "2021-05-29T14:15:05.918883Z",
     "shell.execute_reply": "2021-05-29T14:15:05.917589Z",
     "shell.execute_reply.started": "2021-05-29T14:15:05.898969Z"
    }
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(104, 156)\n",
    "        self.bn1 = nn.BatchNorm1d(156)\n",
    "        \n",
    "        self.linear2 = nn.Linear(156, 208)\n",
    "        self.bn2 = nn.BatchNorm1d(208)\n",
    "        \n",
    "        self.linear3 = nn.Linear(208, 156)\n",
    "        self.bn3 = nn.BatchNorm1d(156)\n",
    "        \n",
    "        self.linear4 = nn.Linear(156, 104)\n",
    "        self.bn4 = nn.BatchNorm1d(104)\n",
    "        \n",
    "        self.linear5 = nn.Linear(104, 64)\n",
    "        self.bn5 = nn.BatchNorm1d(64)\n",
    "\n",
    "\n",
    "        self.final = nn.Linear(64,9)\n",
    "      \n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.bn1(self.linear1(x)))\n",
    "        x = F.leaky_relu(self.bn2(self.linear2(x)))\n",
    "        x = F.leaky_relu(self.bn3(self.linear3(x)))\n",
    "        x = F.leaky_relu(self.bn4(self.linear4(x)))\n",
    "        x = F.leaky_relu(self.bn5(self.linear5(x)))\n",
    "\n",
    "        x = self.final(x)\n",
    "        return x\n",
    "\n",
    "mlp = MLP().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DATA PROCESSING METHODS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-29T14:15:05.921180Z",
     "iopub.status.busy": "2021-05-29T14:15:05.920634Z",
     "iopub.status.idle": "2021-05-29T14:15:05.938169Z",
     "shell.execute_reply": "2021-05-29T14:15:05.936996Z",
     "shell.execute_reply.started": "2021-05-29T14:15:05.921138Z"
    }
   },
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    return (data - np.mean(data, axis=0))/np.std(data, axis=0)\n",
    "\n",
    "audio_train = normalize(audio_train)\n",
    "audio_val = normalize(audio_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-29T14:15:05.940449Z",
     "iopub.status.busy": "2021-05-29T14:15:05.940143Z",
     "iopub.status.idle": "2021-05-29T14:15:05.948864Z",
     "shell.execute_reply": "2021-05-29T14:15:05.947521Z",
     "shell.execute_reply.started": "2021-05-29T14:15:05.940420Z"
    }
   },
   "outputs": [],
   "source": [
    "class AddGaussianNoise:\n",
    "    def __init__(self, mean=0., std=1.):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-29T14:15:05.951517Z",
     "iopub.status.busy": "2021-05-29T14:15:05.950602Z",
     "iopub.status.idle": "2021-05-29T14:15:05.960931Z",
     "shell.execute_reply": "2021-05-29T14:15:05.959721Z",
     "shell.execute_reply.started": "2021-05-29T14:15:05.951471Z"
    }
   },
   "outputs": [],
   "source": [
    "class RandomCircularShift:      \n",
    "    def __call__(self, tensor):\n",
    "        return torch.roll(tensor, 13*np.random.randint(8),dims=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AUDIO DATA TRANSFORMATION** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-29T14:28:09.613354Z",
     "iopub.status.busy": "2021-05-29T14:28:09.612967Z",
     "iopub.status.idle": "2021-05-29T14:28:09.622364Z",
     "shell.execute_reply": "2021-05-29T14:28:09.621126Z",
     "shell.execute_reply.started": "2021-05-29T14:28:09.613324Z"
    }
   },
   "outputs": [],
   "source": [
    "img_train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
    "])\n",
    "img_val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "audio_train_transform = transforms.Compose([\n",
    "    torch.from_numpy,\n",
    "    AddGaussianNoise(0.0, 0.1),\n",
    "    RandomCircularShift()\n",
    "])\n",
    "\n",
    "audio_val_transform = transforms.Compose([\n",
    "    torch.from_numpy,\n",
    "    AddGaussianNoise(0.0, 0.1),\n",
    "    RandomCircularShift()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AUDIO DATA LOADER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-29T14:28:10.018824Z",
     "iopub.status.busy": "2021-05-29T14:28:10.018487Z",
     "iopub.status.idle": "2021-05-29T14:28:10.026183Z",
     "shell.execute_reply": "2021-05-29T14:28:10.024417Z",
     "shell.execute_reply.started": "2021-05-29T14:28:10.018795Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = ImgAudioDataset(root_dir='/kaggle/input/scene-classification-images-and-audio', \n",
    "                                img_data=img_train, audio_data=audio_train, labels=labels_train, img_transform = img_train_transform, audio_transform=audio_train_transform)\n",
    "val_data = ImgAudioDataset(root_dir='/kaggle/input/scene-classification-images-and-audio4', \n",
    "                           img_data=img_val, audio_data=audio_val, labels=labels_val, img_transform = img_val_transform, audio_transform=audio_val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=16, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_data, batch_size=16, shuffle=True, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-29T14:28:10.192292Z",
     "iopub.status.busy": "2021-05-29T14:28:10.191836Z",
     "iopub.status.idle": "2021-05-29T14:28:10.199548Z",
     "shell.execute_reply": "2021-05-29T14:28:10.198097Z",
     "shell.execute_reply.started": "2021-05-29T14:28:10.192262Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(resnext.fc.parameters(), lr=0.001) \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, patience=0,factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-29T14:28:10.358634Z",
     "iopub.status.busy": "2021-05-29T14:28:10.358272Z",
     "iopub.status.idle": "2021-05-29T14:37:36.125516Z",
     "shell.execute_reply": "2021-05-29T14:37:36.124406Z",
     "shell.execute_reply.started": "2021-05-29T14:28:10.358605Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1\n",
      "[1,   200] loss: 2.258\n",
      "[1,   400] loss: 2.273\n",
      "[1,   600] loss: 2.266\n",
      "Epoch accuracy: 0.12163075804710388, Epoch loss: 0.14148957297924278\n",
      "Epoch duration : 57.21779203414917\n",
      "epoch : 2\n",
      "[2,   200] loss: 2.261\n",
      "[2,   400] loss: 2.260\n",
      "[2,   600] loss: 2.269\n",
      "Epoch accuracy: 0.11911892890930176, Epoch loss: 0.1414788984904576\n",
      "Epoch duration : 56.88335061073303\n",
      "epoch : 3\n",
      "[3,   200] loss: 2.267\n",
      "[3,   400] loss: 2.261\n",
      "[3,   600] loss: 2.267\n",
      "Epoch accuracy: 0.11882910132408142, Epoch loss: 0.1416385346317669\n",
      "Epoch duration : 57.19689226150513\n",
      "epoch : 4\n",
      "[4,   200] loss: 2.279\n",
      "[4,   400] loss: 2.260\n",
      "[4,   600] loss: 2.257\n",
      "Epoch accuracy: 0.11766979098320007, Epoch loss: 0.1415878338302901\n",
      "Epoch duration : 56.679038286209106\n",
      "epoch : 5\n",
      "[5,   200] loss: 2.270\n",
      "[5,   400] loss: 2.262\n",
      "[5,   600] loss: 2.268\n",
      "Epoch accuracy: 0.1201816275715828, Epoch loss: 0.14164912354000617\n",
      "Epoch duration : 57.190605878829956\n",
      "epoch : 6\n",
      "[6,   200] loss: 2.261\n",
      "[6,   400] loss: 2.256\n",
      "[6,   600] loss: 2.263\n",
      "Epoch accuracy: 0.1225968524813652, Epoch loss: 0.14119563842324195\n",
      "Epoch duration : 57.55880641937256\n",
      "epoch : 7\n",
      "[7,   200] loss: 2.262\n",
      "[7,   400] loss: 2.266\n",
      "[7,   600] loss: 2.270\n",
      "Epoch accuracy: 0.11670369654893875, Epoch loss: 0.14169621319024991\n",
      "Epoch duration : 55.64352798461914\n",
      "epoch : 8\n",
      "[8,   200] loss: 2.269\n",
      "[8,   400] loss: 2.259\n",
      "[8,   600] loss: 2.261\n",
      "Epoch accuracy: 0.1209544986486435, Epoch loss: 0.1415253939369245\n",
      "Epoch duration : 56.79522633552551\n",
      "epoch : 9\n",
      "[9,   200] loss: 2.266\n",
      "[9,   400] loss: 2.262\n",
      "[9,   600] loss: 2.268\n",
      "Epoch accuracy: 0.11911892890930176, Epoch loss: 0.14146788804822008\n",
      "Epoch duration : 55.548588037490845\n",
      "epoch : 10\n",
      "[10,   200] loss: 2.252\n",
      "[10,   400] loss: 2.263\n",
      "[10,   600] loss: 2.261\n",
      "Epoch accuracy: 0.11960197240114212, Epoch loss: 0.14138008616256456\n",
      "Epoch duration : 55.02361536026001\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "data_type = 'audio'\n",
    "mlp, loss_vals, train_acc_vals, val_acc_vals = train(data_type, mlp, train_loader, val_loader, criterion, optimizer, scheduler, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
